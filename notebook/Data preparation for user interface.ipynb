{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for user interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing TFIDF is time consuming for such a large data set. It is important to make our user interface fast and accurate. We decided to pre-precess and pre-calculate the needed data and store it in the user interface folder. \"weights_dict_bytime\" and \"doc_dict\" are what we finally use in our web-app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Fasttext Model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model        \n",
    "model = fasttext.load_model(\"model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormatConvert(df):\n",
    "    data.summary.fillna(value=' ', inplace=True)\n",
    "    comments = list(df['summary']+': ' + df['reviewText'])\n",
    "    comments = [w.replace('\\n', '') for w in comments]\n",
    "    return comments\n",
    "\n",
    "def readJson(filePath):\n",
    "#     filePath = \"data/\" + filePath\n",
    "#     with open(filePath) as data: \n",
    "    data = json.load(filePath)\n",
    "    data = pd.DataFrame(data)        \n",
    "    # Convert the string time back to datetime\n",
    "    data[\"reviewTime\"] = pd.to_datetime(data.reviewTime)\n",
    "    data = data[data['reviewText'].notna()]\n",
    "    return data\n",
    "\n",
    "def subsetData(data, pct):\n",
    "    if data.shape[0] > 500000:\n",
    "        data = data.sample(frac = pct)\n",
    "    return data \n",
    "\n",
    "def saveJson(data, file_name):\n",
    "#     file_name = 'data/' + file_name    \n",
    "    # Store the datetime as string time in order to store in json\n",
    "    data[\"reviewTime\"] = data.reviewTime.dt.strftime('%Y-%m-%d')\n",
    "    data.to_json(file_name, orient='records')\n",
    "#     print(\"Saving..., file name:\", file_name)\n",
    "\n",
    "def textExtract_str(data):\n",
    "    data = data[data['reviewText'].notna()]\n",
    "    rawtext =  ' '. join(list(data['reviewText']))\n",
    "    return rawtext\n",
    "\n",
    "def removePuncLower(s):\n",
    "    re_tok = re.compile(r'[^\\w]+')\n",
    "    s = re_tok.sub(' ', s).lower()\n",
    "    return s\n",
    "\n",
    "def computeTFIDF(corpus, columnnames, dfmax):    \n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', max_df = dfmax, use_idf = True)\n",
    "    print(\"transforming...\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"getting feature name...\")\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(\"densing list...\")\n",
    "    dense = X.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"reshaping list...\")\n",
    "    new_list = [list(x) for x in zip(*denselist)]\n",
    "    print(\"converting to data frame...\")\n",
    "    df = pd.DataFrame(new_list, index=feature_names, columns = columnnames)\n",
    "    return df\n",
    "\n",
    "def save_obj(obj, path, name ):\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'data'\n",
    "filelist = os.listdir(filePath)\n",
    "for i in range(len(filelist)):\n",
    "    filelist[i] = filelist[i][:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label to all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filelist)):\n",
    "# for i in range(1):\n",
    "    with open('data/' + filelist[i] + '.json') as f:\n",
    "        print(\"reading \" + filelist[i] + '...')\n",
    "        data = readJson(f)\n",
    "        print(\"subseting \" + filelist[i] + '...')\n",
    "        data = subsetData(data, 0.1)\n",
    "        print(\"processing \" + filelist[i] + '...')\n",
    "        text = FormatConvert(data)\n",
    " ####### modeling ####### \n",
    "        # Use the predict function 0: Neg, 1: Pos\n",
    "        pred = model.predict(text)\n",
    "        pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]\n",
    "        data['pred_labels'] = pred_labels\n",
    "        newpath = 'data/subsets_data/' + filelist[i] + '.json'\n",
    "        print(\"saving \" + newpath)\n",
    "        saveJson(data, newpath)\n",
    "        f.close()\n",
    "        os.remove('data/' + filelist[i] + '.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save TFIDF as dictionary for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = [24,12,6,3]\n",
    "senti_label = ['pos', 'neg', 'all']\n",
    "\n",
    "weights_dict = defaultdict(dict)\n",
    "for t in time_period:\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    doc_list = []\n",
    "    weights_dict[t] = defaultdict(dict)\n",
    "    print(\"processing period %s\" % (t))\n",
    "    for i in range(len(filelist)):\n",
    "    # for i in range(1):\n",
    "        weights_dict[t][filelist[i]] = defaultdict(dict)\n",
    "        with open(filePath + filelist[i] + '.json') as f:\n",
    "            print(\"reading \" + filelist[i] + '...')\n",
    "            data = readJson(f)\n",
    "            data = data[data.reviewTime > max(data.reviewTime) - timedelta(days=30 * t)]\n",
    "            pos_text = textExtract_str(data[data['pred_labels'] == 1])\n",
    "            pos_text = removePuncLower(pos_text)\n",
    "            pos_list.append(pos_text)\n",
    "            neg_text = textExtract_str(data[data['pred_labels'] == 0])        \n",
    "            neg_text = removePuncLower(neg_text)\n",
    "            neg_list.append(neg_text)\n",
    "            text = textExtract_str(data)\n",
    "            text = removePuncLower(text)\n",
    "            doc_list.append(text)\n",
    "        \n",
    "    pos_tfidf_df = computeTFIDF(pos_list, filelist, 0.99)\n",
    "    neg_tfidf_df = computeTFIDF(neg_list, filelist, 0.99)\n",
    "    tfidf_df = computeTFIDF(doc_list, filelist, 0.99)\n",
    "    \n",
    "    for i in range(len(filelist)):\n",
    "        top10 = tfidf_df[filelist[i]].nlargest(25).index\n",
    "        top10value = list(tfidf_df[filelist[i]].nlargest(25))\n",
    "        weights = {top10[i]:top10value[i] for i in range(25)}\n",
    "        weights_dict[t][filelist[i]]['all'] = weights\n",
    "\n",
    "        top10 = pos_tfidf_df[filelist[i]].nlargest(25).index\n",
    "        top10value = list(pos_tfidf_df[filelist[i]].nlargest(25))\n",
    "        weights = {top10[i]:top10value[i] for i in range(25)}   \n",
    "        weights_dict[t][filelist[i]]['pos'] = weights\n",
    "\n",
    "        top10 = neg_tfidf_df[filelist[i]].nlargest(25).index\n",
    "        top10value = list(neg_tfidf_df[filelist[i]].nlargest(25))\n",
    "        weights = {top10[i]:top10value[i] for i in range(25)} \n",
    "        weights_dict[t][filelist[i]]['neg'] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(weights_dict, 'data/tfidf/', 'weights_dict_bytime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf for uploaded file \n",
    "using the most recent 3 month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = defaultdict(list)\n",
    "    \n",
    "for i in range(len(filelist)):\n",
    "# for i in range(1):\n",
    "    with open(filePath + filelist[i] + '.json') as f:\n",
    "        print(\"reading \" + filelist[i] + '...')\n",
    "        data = readJson(f)\n",
    "        data = data[data.reviewTime > max(data.reviewTime) - timedelta(days=30 * 3)]\n",
    "        pos_text = textExtract_str(data[data['pred_labels'] == 1])\n",
    "        pos_text = removePuncLower(pos_text)\n",
    "        doc_dict['pos'].append(pos_text)\n",
    "        neg_text = textExtract_str(data[data['pred_labels'] == 0])        \n",
    "        neg_text = removePuncLower(neg_text)\n",
    "        doc_dict['neg'].append(neg_text)\n",
    "        text = textExtract_str(data)\n",
    "        text = removePuncLower(text)\n",
    "        doc_dict['all'].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(weights_dict, 'data/tfidf/', 'doc_dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
